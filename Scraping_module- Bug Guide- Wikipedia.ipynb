{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Installation of BeautifulSoup and requests package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib\n",
    "import time\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "driver = webdriver.Firefox(executable_path=r'P:\\Desktop\\geckodriver-v0.23.0-win64\\geckodriver.exe')\n",
    "url = \"https://inaturalist.nz/observations/18639810\"\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "soup = bs(html,  'lxml')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original GBIF Exotic data in excel format. Reading excel file using pandas \n",
    "data = pd.read_excel(\"T:/projects/2018/SCION/Stream 2/Outer Join Exotic_GBIF Data/181208_Exotic_Gbif_Full_Species.xlsx\",\n",
    "                    sheet_name= 'Text Mining Sheet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Original_species_name', 'LR_Text', 'BG_Text', 'Wiki_Text', 'Flag',\n",
       "       'Full Text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2641"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = data['Original_species_name']\n",
    "list1 = list1.unique()\n",
    "data['Original_species_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     wikipedia.page('Acallopais rudis')\n",
    "# except:\n",
    "#     print(\"page not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Wikipedia for decription as it contains information about size and other behavioural properties which might be useful for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"\"\n",
    "new_x = []\n",
    "description = []\n",
    "Distribution_habitat = []\n",
    "parasite_host = []\n",
    "pest_species = []\n",
    "for i in list1:\n",
    "    try:\n",
    "        url_link = wikipedia.page(i)\n",
    "        x = url_link.content\n",
    "        new_x = x.split(\"\\n\")\n",
    "        try:\n",
    "            d_index = new_x.index(\"== Description ==\")\n",
    "            life_index = new_x.index('== Life cycle ==')\n",
    "            description.append(new_x[d_index:life_index])\n",
    "            try:\n",
    "                habitat_index = new_x.index(\"== Distribution and habitat ==\")\n",
    "                parasite_index = new_x.index(\"== As parasite host ==\")\n",
    "                Distribution_habitat.append(new_x[habitat_index:parasite_index])\n",
    "                try:\n",
    "                    parasite_index = new_x.index(\"== As parasite host ==\")\n",
    "                    pest_index = new_x.index('== As a pest species ==')\n",
    "                    parasite_host.append(new_x[parasite_index:pest_index])\n",
    "                    try:\n",
    "                        pest_index = new_x.index('== As a pest species ==')\n",
    "                        pest_species.append(new_x[pest_index:pest_index +1 ])\n",
    "                    except:\n",
    "                        pest_species.append(\"NA\")\n",
    "                        \n",
    "                        \n",
    "                except:\n",
    "                    parasite_host.append(\"NA\")\n",
    "            except:\n",
    "                Distribution_habitat.append(\"NA\")\n",
    "            \n",
    "            \n",
    "            \n",
    "        except:\n",
    "            description.append(\"NA\")\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        description.append(\"page not found\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wk = pd.DataFrame({'Original species name':list1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wk['wiki_description']=description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wk.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Wiki_Description with Exotic_GBIF_All_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = data.merge(df_wk, on=['Original species name'], how = \"left\" )\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bug Guide Website Scraping - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Original species name':list1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = data['Original species name']\n",
    "list1 = list1.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_values(text ,value):\n",
    "    main_list = ''\n",
    "    temp_list = []\n",
    "\n",
    "    for list_of_text in text.split(\"\\n\"):\n",
    "        if list_of_text.startswith(value):\n",
    "            temp_list.append(list_of_text)\n",
    "\n",
    "    if len(temp_list) != 0:\n",
    "        main_list = temp_list[0]\n",
    "    else:\n",
    "        main_list = \"NA\"\n",
    "\n",
    "    return main_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents_from_text(link):\n",
    "    print(link)\n",
    "    r = requests.get(link)\n",
    "    soup = bs(r.text,'lxml')\n",
    "    text = soup.text\n",
    "    size.append(get_list_values(text, 'Size'))\n",
    "    Identification.append(get_list_values(text,'Identification'))\n",
    "    Range.append(get_list_values(text, 'Range'))\n",
    "    Season.append(get_list_values(text , 'Season'))\n",
    "    Life_cycle.append(get_list_values(text , 'Life Cycle'))\n",
    "    Food.append(get_list_values(text , 'Food'))\n",
    "    Remarks.append(get_list_values(text , 'Remarks'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_collection = []\n",
    "size = []\n",
    "Identification = []\n",
    "Range = []\n",
    "Food = []\n",
    "Remarks = []\n",
    "Life_cycle = []\n",
    "Season = []\n",
    "def create_list_for_link():\n",
    "    link_list = []\n",
    "    for link in soup.find_all(\"a\"):\n",
    "    #print(link.attrs)\n",
    "        link_list.append(list(link.attrs.values()))\n",
    "    return link_list\n",
    "\n",
    "def get_info(index_link):\n",
    "    #info_link_list = []\n",
    "    rr = requests.get(index_link)\n",
    "    soup = bs(rr.text, 'lxml')\n",
    "    info_link = soup.find('a',string = 'Info')\n",
    "    if info_link is None:\n",
    "        info_link = index_link\n",
    "    else:\n",
    "        info_link = info_link.get('href')\n",
    "    return(info_link)\n",
    "\n",
    "\n",
    "for i in list1:\n",
    "    x = i.split(\" \")\n",
    "    r= requests.get('https://bugguide.net/index.php?q=search&keys=' + x[0]+ '+' + x[1] + '&search=Search')\n",
    "    #r= requests.get('https://bugguide.net/index.php?q=search&keys=Acyrthosiphon+kondoi&search=Search')\n",
    "    soup = bs(r.text, \"lxml\")\n",
    "    link_list_new = create_list_for_link()\n",
    "    #print(link_list_new)\n",
    "\n",
    "    index_value = link_list_new.index(['https://bugguide.net/adv_search/bgsearch.php'])\n",
    "    index_link = link_list_new[index_value+1]\n",
    "    first_link = index_link[0]\n",
    "    info_link_returned = get_info(first_link)\n",
    "    get_contents_from_text(info_link_returned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://inaturalist.nz/taxa/search?q=spoladea-recurvalis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BG_size']=size\n",
    "df['BG_Season']=Season\n",
    "df['BG_Life_cycle']=Life_cycle\n",
    "df['BG_Remarks']=Remarks\n",
    "df['BG_Food'] = Food\n",
    "df['BG_Identification']=Identification\n",
    "df['BG_Range']=Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(df, on=['Original species name'], how = \"left\" )\n",
    "data.head(5)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saved excel file with added information from bug guide, wikipedia and merged into original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_excel('T:/projects/2018/SCION/Stream 2/All Exotic GBIF Merged Data/Exotic_Gbif_BG__Wiki_All_Data_Species1.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating size of the bugs which we scraped from bug guide. As it is in text form we, have used regular expression to extract information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"T:/projects/2018/SCION/Stream 2/All Exotic GBIF Merged Data/Exotic_Gbif_BG__Wiki_All_Data_Species1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Size(mm)'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_size_na = data[np.isnan(data['Size(mm)']) == True]\n",
    "data_size_na['BG_size'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_data = data_size_na[['BG_size','Original species name']].reset_index().drop(['index'], axis =1)\n",
    "size_data.head()\n",
    "#pd.isnull(size_data.iloc[]['BG_size'])== True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\\d*[.]?\\d*[-<]?\\d*[.]?\\d*')\n",
    "\n",
    "matches = re.findall(pattern,size_data.iloc[14]['BG_size'])\n",
    "type(matches)\n",
    "list1 = []\n",
    "#print(matches.pop(''))\n",
    "# matches = pattern.finditer(size_data.iloc[14]['BG_size'])\n",
    "# print(matches)\n",
    "for match in matches:\n",
    "    if match !='':\n",
    "        list1.append(match)\n",
    "        print(list1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# pattern = re.compile(r'\\d*[.]?\\d*[-<]?\\d*[.]?\\d*')\n",
    "# list1 = []\n",
    "# single_list = []\n",
    "# for i in range(len(size_data['BG_size'])):\n",
    "#     if pd.isnull(size_data.iloc[i]['BG_size']) == False:\n",
    "#         matches = re.findall(pattern,size_data.iloc[i]['BG_size'])\n",
    "#         #print(matches)\n",
    "#         for match in matches:\n",
    "#             if match !='':\n",
    "#                 #print(match)\n",
    "#                 list1.append(match)\n",
    "#         single_list.append(list1[0])\n",
    "                \n",
    "#     else:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def create_single_list(matches):\n",
    "    single_list = []\n",
    "    for match in matches:\n",
    "        if match != '':\n",
    "            single_list.append(match)\n",
    "            #print(single_list[0])\n",
    "    return single_list[0]\n",
    "    \n",
    "pattern = re.compile(r'\\d*[.]?\\d*[-<]?\\d*[.]?\\d*')\n",
    "list1 = []\n",
    "for i in range(len(size_data['BG_size'])):\n",
    "    if pd.isnull(size_data.iloc[i]['BG_size']) == False:\n",
    "        matches = re.findall(pattern, size_data.iloc[i]['BG_size'])\n",
    "        #print(matches)\n",
    "        list_value = create_single_list(matches)\n",
    "        list1.append(list_value)\n",
    "    else:\n",
    "        list1.append(None)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '<3'.strip('<')\n",
    "# list1[1].strip('<').split('-')\n",
    "range(len(list1))\n",
    "#pd.isnull(list1[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size_data['BG_avg_Size'] = np.nan\n",
    "list_new = []\n",
    "\n",
    "for i in range(len(list1)):\n",
    "    if pd.isnull(list1[i]) != True:\n",
    "        x = list1[i].strip('<').split('-')\n",
    "        if len(x) <2 :\n",
    "            list_new.append(float(x[0]))\n",
    "        else:\n",
    "            avg_size = float(x[0])+float(x[1])/2\n",
    "            list_new.append(avg_size)\n",
    "    else:\n",
    "        list_new.append(np.nan)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_data['BG_avg_Size'] = list_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = pd.merge(data, size_data, how= 'left', on= ['Original species name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new.to_excel('T:/projects/2018/SCION/Stream 2/All Exotic GBIF Merged Data/Exotic_Gbif_BG__Wiki_All_Data_Species1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia Scraping on entire list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Abgrallaspis cyanophylli', 'Acallopais rudis',\n",
       "       'Acantholybas brunneus', ..., 'Pediculus capitis',\n",
       "       'Pseudoscorpionida', 'Liposcelis divinatorius'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'T:/projects/2018/SCION/Stream 2/text mining/0_Text_Priophorus_brullei/morio.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-d23904fa838a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m45\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/0_Text_{}.txt'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhome_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecies_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'a+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m                 \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[1;31m#print(a)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'T:/projects/2018/SCION/Stream 2/text mining/0_Text_Priophorus_brullei/morio.txt'"
     ]
    }
   ],
   "source": [
    "text_wiki = []\n",
    "\n",
    "home_dir = \"T:/projects/2018/SCION/Stream 2/text mining\"\n",
    "for name in list1:\n",
    "    text_from_wiki = ''\n",
    "    if len(name.split())>1:\n",
    "        species_name = name.split()[0] + '_' + name.split()[1]\n",
    "    else:\n",
    "        species_name = name\n",
    "    #print(species_name)\n",
    "    r = requests.get(\"https://en.wikipedia.org/wiki/\"+species_name)\n",
    "    soup = bs(r.text, 'lxml')\n",
    "    for text in soup.find_all('p'):\n",
    "        text_from_wiki += text.getText()\n",
    "        if len(text.getText()) != 45:\n",
    "            with open('{}/{}.txt'.format(home_dir, species_name),'a+', encoding= 'utf-8') as f:\n",
    "                a = text.getText()\n",
    "                #print(a)\n",
    "                f.write(a)\n",
    "        elif len(text.getText()) == 45:\n",
    "            with open('{}/0_Text_{}.txt'.format(home_dir, species_name),'a+', encoding= 'utf-8') as f:\n",
    "                a = text.getText()\n",
    "                #print(a)\n",
    "                f.write(a)\n",
    "    text_wiki.append(text_from_wiki)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "with io.open('log.txt', 'a', encoding='utf8') as logfile:\n",
    "    for tr in soup.find_all('tr')[2:]:\n",
    "        tds = tr.find_all('td')\n",
    "        logfile.write(u\"%s, %s, %s\\n\" % (tds[0].text, tds[1].text, tds[2].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
