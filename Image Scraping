{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(executable_path=r'P:\\Desktop\\geckodriver-v0.23.0-win64\\geckodriver.exe')\n",
    "url = \"https://inaturalist.nz/observations/10478789\"\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "soup = bs(html,  'lxml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://static.inaturalist.org/photos/14568117/large.jpeg?1522273669\n",
      "Downloading https://static.inaturalist.org/photos/14568117/large.jpeg?1522273669 ---> large.jpeg\n"
     ]
    }
   ],
   "source": [
    "links = soup.find('div', class_= 'image-gallery-image').find_all('img', src=True)\n",
    "for link in links:\n",
    "    imgUrl = link.get('src')\n",
    "    print(imgUrl)\n",
    "    download_file(imgUrl)\n",
    "    #urllib.(imgUrl, os.path.basename(imgUrl))\n",
    "    \n",
    "\n",
    "# for link in links:\n",
    "#     timestamp = time.asctime() \n",
    "#     txt = open('%s.jpg' % timestamp, \"wb\")\n",
    "#     link = link[\"src\"].split(\"src=\")[-1]\n",
    "#     download_img = urllib2.urlopen(link)\n",
    "#     txt.write(download_img.read())\n",
    "\n",
    "#     txt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url):\n",
    "    local_filename = url.split('/')[-1]\n",
    "    local_filename = local_filename.split(\"?\")[0]\n",
    "    print(\"Downloading {} ---> {}\".format(url, local_filename))\n",
    "    # NOTE the stream=True parameter\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(local_filename, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024): \n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'large.jpeg'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"large.jpeg?1522273669\".split(\"?\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Insecta\n"
     ]
    }
   ],
   "source": [
    "content_factors = soup.find_all(\"p\", class_ = \"title\")\n",
    "for i in content_factors:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_italics = soup.find_all(\"span\", class_ = \"textItalics\")\n",
    "for j in content_italics:\n",
    "    print(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insect = {}\n",
    "for i in range(len(content_italics)):\n",
    "    insect[content_factors[i].text] = content_italics[i].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(insect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_values(text ,value):\n",
    "    main_list = ''\n",
    "    temp_list = []\n",
    "\n",
    "    for list_of_text in text.split(\"\\n\"):\n",
    "        if list_of_text.startswith(value):\n",
    "            temp_list.append(list_of_text)\n",
    "\n",
    "    if len(temp_list) != 0:\n",
    "        main_list = temp_list[0]\n",
    "    else:\n",
    "        main_list = \"NA\"\n",
    "\n",
    "    return main_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def get_contents_from_text(link):\n",
    "    print(link)\n",
    "    r = requests.get(link)\n",
    "    soup = bs(r.text,'lxml')\n",
    "    text = soup.text\n",
    "    size.append(get_list_values(text, 'Size'))\n",
    "    Identification.append(get_list_values(text,'Identification'))\n",
    "    Range.append(get_list_values(text, 'Range'))\n",
    "    Season.append(get_list_values(text , 'Season'))\n",
    "    Life_cycle.append(get_list_values(text , 'Life Cycle'))\n",
    "    Food.append(get_list_values(text , 'Food'))\n",
    "    Remarks.append(get_list_values(text , 'Remarks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collection = []\n",
    "size = []\n",
    "Identification = []\n",
    "Range = []\n",
    "Food = []\n",
    "Remarks = []\n",
    "Life_cycle = []\n",
    "Season = []\n",
    "def create_list_for_link():\n",
    "    link_list = []\n",
    "    for link in soup.find_all(\"a\"):\n",
    "    #print(link.attrs)\n",
    "        link_list.append(list(link.attrs.values()))\n",
    "    return link_list\n",
    "\n",
    "def get_info(index_link):\n",
    "    #info_link_list = []\n",
    "    rr = requests.get(index_link)\n",
    "    soup = bs(rr.text, 'lxml')\n",
    "    info_link = soup.find('a',string = 'Info')\n",
    "    if info_link is None:\n",
    "        info_link = index_link\n",
    "    else:\n",
    "        info_link = info_link.get('href')\n",
    "    return(info_link)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in list1:\n",
    "    x = i.split(\" \")\n",
    "    r= requests.get('https://bugguide.net/index.php?q=search&keys=' + x[0]+ '+' + x[1] + '&search=Search')\n",
    "    #r= requests.get('https://bugguide.net/index.php?q=search&keys=Acyrthosiphon+kondoi&search=Search')\n",
    "    soup = bs(r.text, \"lxml\")\n",
    "    link_list_new = create_list_for_link()\n",
    "    #print(link_list_new)\n",
    "\n",
    "    index_value = link_list_new.index(['https://bugguide.net/adv_search/bgsearch.php'])\n",
    "    index_link = link_list_new[index_value+1]\n",
    "    first_link = index_link[0]\n",
    "    info_link_returned = get_info(first_link)\n",
    "\n",
    "#         rr_info = requests.get(info_link_returned)\n",
    "#         soup = bs(rr_info.text, 'lxml')\n",
    "    get_contents_from_text(info_link_returned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BG_size']=size\n",
    "df['BG_Season']=Season\n",
    "df['BG_Life_cycle']=Life_cycle\n",
    "df['BG_Remarks']=Remarks\n",
    "df['BG_Food'] = Food\n",
    "df['BG_Identification']=Identification\n",
    "df['BG_Range']=Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
